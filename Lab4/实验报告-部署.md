# Ceph的部署

实验4 实验报告（Part1）

### 运行环境

VMWare Workstation Pro17虚拟机。使用三台虚拟机`ceph0` `ceph1` `ceph2`组成Ceph集群，三台虚拟机均为Ubuntu 22.04.3 LTS，硬件配置相同如下：

- 4GB内存；
- 2个单核处理器，物理机CPU为Core i5-11400H；
- 三SCSI硬盘，其中主硬盘32G用于安装Ubuntu系统本身和必要软件；两个16G新硬盘完全交给Ceph；
- 虚拟机之间通过Host-Only网络连接，使用DHCP，将重新分配间隔调至最长，这样可以认为在实验时间内为静态IP（当然，通过修改DHCP配置文件也可以直接分配静态IP段）。该网络中三台机器的IP分别为`192.168.244.129` - `192.168.244.131`。

### 预处理

注意：不加说明，以下操作均在ceph0节点的root用户下进行（使用`sudo -i`进入）

- 首先`sudo vim /etc/hosts`将三台机器的IP互相加入到HOSTS文件中。
- 需要安装的必要软件有`openssh-server docker net-tools`等。
- 先使用`ssh-copy-id`将ceph0的公钥拷贝到ceph1, ceph2的`authorized_keys`文件中。注意由于安全原因，该命令并不能直接拷贝到root用户，但我们可以用非root用户作为中转站。
- Ceph镜像从quay.io下载较慢，可以先从USTC镜像将其pull下来，这样cephadm会直接使用该镜像。（另两台虚拟机也可以先pull下来）

### 单机部署

过去使用较多的`ceph-deploy`工具事实上已经被弃用了，这里我们使用Ceph官方推荐的cephadm工具完成部署。

- cephadm可以通过包括apt在内的很多方式安装。这里直接使用apt即可。
- 理论上使用`cephadm bootstrap --mon-ip 192.168.244.129`（mon-ip即为自身ip）一个命令即可完成单机部署。实际上运行过程中可能出现一些问题，不过相对来说都比较好解决；完成bootstrap之后，cephadm会提供`ceph dashboard`的网址（一般为`localhost:8443`）、初始账号密码，以登录dashboard，以通过图形化的方式监控、调整集群。
  - 图形化界面的功能相当有限，用来作为获取集群当前状态信息的工具即可，不应作为管理集群的主要方式。
- 之后运行`cephadm install ceph-common`，即可正常使用`ceph`命令，对集群进行管理。

### 多机部署

单机不应该是一个集群的正常状态。事实上，在上面的步骤完成之后，`ceph status`会对集群的健康进行警告，报告包括OSD数量<3等问题。为了解决这个问题，我们首先应当为集群添加新的机器。

- 添加新的机器前，需要让Ceph能够控制远程机器，因此再用一遍`ssh-copy-id`：
  `ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph1`(ceph2)
  - 在上一次`ssh-copy-id`成功进行的前提下，这次运行命令应该能直接顺利完成。
- 之后运行`ceph orch host add ceph1 *<ip>* --labels _admin`，即可将另两台机器作为admin加入到集群。
- 接下来需要给集群加入存储服务。使用`ceph orch apply osd --all-available-devices`，告知Ceph计划在所有空闲磁盘上部署OSD。之后，部署在六个硬盘上的OSD会逐个加入到集群中。完成这一步之后，其实已经完成部署了一个健康的集群。
  ![health](src\health.png)
  ![OSDs](src\OSDs.png)